{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a23209-cd9f-4a00-8836-ff0eeed29878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pred_method = \"boltz1\"\n",
    "folder_path = f\"{pred_method}\"\n",
    "\n",
    "## file hierarchy should be like this\n",
    "# af2\n",
    "# ├── 7ar0_B_A_boltz1\n",
    "# │   ├── predictions\n",
    "# │       ├── result\n",
    "# │           ├── xxx.pdb\n",
    "# │           ├── ...\n",
    "# ├── 7bnv_H_L_A_af2\n",
    "# │   ├── predictions\n",
    "# │       ├── result\n",
    "# │           ├── ...\n",
    "\n",
    "complex_list = [\n",
    "    f for f in os.listdir(folder_path)\n",
    "    if os.path.isdir(os.path.join(folder_path, f)) and not f.startswith(\".\")\n",
    "]\n",
    "print(complex_list)  # should print prediction folders ['7ar0_B_A_af2', '...', '...']\n",
    "\n",
    "original_directory = \"path/to/native_PDBs\" ## Native PDB directory\n",
    "\n",
    "## create results folder\n",
    "result_path = \"path/to/results_folder\"\n",
    "dockq_output = f\"{pred_method}_dockq_fnat_scores.csv\"  ## dockq output\n",
    "pdockq_output = f\"{pred_method}_pdockq2_fit.csv\" ## pdockq2 output\n",
    "combined = f\"{pred_method}_combined.csv\" ### combined results with dockq_pdock2 and model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200560f2-9e7b-43d3-95ea-5306baf83678",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run dockq function\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import glob\n",
    "from DockQ.DockQ import load_PDB, run_on_all_native_interfaces\n",
    "from statistics import mean\n",
    "\n",
    "def merge_chains(model, chains_to_merge):\n",
    "    \"\"\"\n",
    "    Merges specified chains in the given model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Bio.PDB.Structure\n",
    "        The model in which the chains are to be merged.\n",
    "    chains_to_merge : list of str\n",
    "        The list of chain IDs to be merged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : Bio.PDB.Structure\n",
    "        The model with the specified chains merged.\n",
    "    \"\"\"\n",
    "    print(f\"Merging chains {chains_to_merge} in model\")\n",
    "    for chain in chains_to_merge[1:]:\n",
    "        for res in list(model[chain]):\n",
    "            res.id = (chains_to_merge[0], res.id[1], res.id[2])\n",
    "            model[chains_to_merge[0]].add(res)\n",
    "        model.detach_child(chain)\n",
    "    model[chains_to_merge[0]].id = \"\".join(chains_to_merge)\n",
    "    return model\n",
    "\n",
    "def calculate_dockq(model, native, chain_map):\n",
    "    \"\"\"\n",
    "    Calculates DockQ scores for the given model and native structures based on the chain map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Bio.PDB.Structure\n",
    "        The model structure.\n",
    "    native : Bio.PDB.Structure\n",
    "        The native structure.\n",
    "    chain_map : dict\n",
    "        The mapping of chains between model and native structures.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        The results containing various DockQ metrics.\n",
    "    dockq_score : float\n",
    "        The DockQ score.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating DockQ score with chain_map: {chain_map}\")\n",
    "    results, dockq_score = run_on_all_native_interfaces(model, native, chain_map=chain_map)\n",
    "    return results, dockq_score\n",
    "\n",
    "def process_models(models):\n",
    "    \"\"\"\n",
    "    Processes the provided models and calculates DockQ scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list of tuple\n",
    "        List of tuples where each tuple contains the model file path and native file path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_list : list of tuple\n",
    "        List of tuples containing model_id, DockQ, DockQ_F1, and fnat scores.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    for model_file, native_file in models:\n",
    "        print(f\"Processing model: {model_file}, native: {native_file}\")\n",
    "        model_id = os.path.basename(model_file).split(\".\")[0]\n",
    "        \n",
    "        model = load_PDB(model_file)\n",
    "        native = load_PDB(native_file)\n",
    "\n",
    "        chain_ids = list(model.child_dict.keys())\n",
    "        print(chain_ids)\n",
    "        native_chain_ids = list(native.child_dict.keys())\n",
    "\n",
    "        if len(chain_ids) == 3:\n",
    "            print(f\"Model {model_id} has 3 chains: {chain_ids}\")\n",
    "            \n",
    "            # Merge A and B chains and recalculate\n",
    "            model_merged = merge_chains(model, chain_ids[:2])\n",
    "            print(model_merged)\n",
    "            native_merged = merge_chains(native, native_chain_ids[:2])\n",
    "            chain_map_merged = {native_chain_ids[2]: chain_ids[2], \"\".join(native_chain_ids[:2]): \"\".join(chain_ids[:2])}\n",
    "            results_merged, dockq_score_merged = calculate_dockq(model_merged, native_merged, chain_map_merged)\n",
    "            merged_result = results_merged[list(results_merged.keys())[0]]\n",
    "            results_list.append((model_id, merged_result['DockQ'], merged_result['fnat'],\n",
    "                                merged_result['iRMSD'], merged_result['LRMSD'],merged_result['F1']))\n",
    "\n",
    "        elif len(chain_ids) == 2:\n",
    "            print(f\"Model {model_id} has 2 chains: {chain_ids}\")\n",
    "            chain_map = {native_chain_ids[0]: chain_ids[0], native_chain_ids[1]: chain_ids[1]}\n",
    "            results, dockq_score = calculate_dockq(model, native, chain_map)\n",
    "            results_list.append((model_id, results[list(results.keys())[0]]['DockQ'], results[list(results.keys())[0]]['fnat'],\n",
    "                                results[list(results.keys())[0]]['iRMSD'], results[list(results.keys())[0]]['LRMSD'],\n",
    "                                results[list(results.keys())[0]]['F1']))\n",
    "\n",
    "        else:\n",
    "            print(f\"Model {model_id} does not have 2 or 3 chains: {chain_ids}, skipping.\")\n",
    "            continue\n",
    "    return results_list\n",
    "\n",
    "def save_results_to_csv(results, filename):\n",
    "    print(f\"Saving results to CSV file: {filename}\")\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['model_id', 'DockQ', 'fnat',\"iRMSD\",\"LRMSD\",\"F1\"])\n",
    "\n",
    "        for model_id, dockq, fnat, irms, lrms, f1 in results:\n",
    "            print(f\"Writing row: {model_id}, {dockq}, {fnat}, {irms},{lrms},{f1}\")\n",
    "            writer.writerow([model_id, dockq, fnat, irms, lrms,f1])   \n",
    "\n",
    "def main(directory, original_directory):\n",
    "    \"\"\"\n",
    "    Main function to find PDB files, process models, and save results to a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        Path to the directory containing PDB files to be analyzed.\n",
    "    original_directory : str\n",
    "        Path to the directory containing original PDB files for comparison.\n",
    "\n",
    "    Returns\n",
    "    -------                                                                                                                                                                                                                                                                                                                                    \n",
    "    None\n",
    "    \"\"\"\n",
    "    pdb_files = glob.glob(f\"{directory}/predictions/result/result_model_*.pdb\")\n",
    "    if not pdb_files:\n",
    "        print(f\"No PDB files found in directory: {directory}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found PDB files: {pdb_files}\")\n",
    "    models = []\n",
    "\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_id_chains = os.path.basename(directory)\n",
    "        pdb_id_chains= directory.split(\"/\")[-1].replace(\"_boltz1\",\"\")\n",
    "        print(pdb_id_chains)\n",
    "        print(f\"Searching for original files for: {pdb_id_chains}\")\n",
    "        original_pdb_files = glob.glob(f\"{original_directory}/{pdb_id_chains}.pdb\")  # Find matching original PDB files\n",
    "        print(f\"Found original PDB files: {original_pdb_files} for {pdb_file}\")\n",
    "        if not original_pdb_files:\n",
    "            print(f\"No matching original PDB file found for {pdb_file}, skipping.\")\n",
    "            continue\n",
    "        for original_pdb_file in original_pdb_files:\n",
    "            models.append((pdb_file, original_pdb_file))\n",
    "            print(f\"Adding model-native pair: {pdb_file}, {original_pdb_file}\")\n",
    "\n",
    "    if not models:\n",
    "        print(\"No valid model-native pairs found. Exiting.\")\n",
    "        return\n",
    "    os.chdir(result_path)\n",
    "    results = process_models(models)\n",
    "    save_results_to_csv(results, str(pdb_id_chains)+'_dockq_fnat_scores.csv')\n",
    "    for model_id, dockq, fnat, irms, lrms, f1 in results:\n",
    "        print(f\"Writing row: {model_id}, {dockq}, {fnat}, {irms},{lrms},{f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3e61e-8d2f-468a-9530-810468fe3e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "for i in complex_list:\n",
    "    fpath = f\"{folder_path}/{i}\"\n",
    "    directory = os.path.expanduser(fpath)\n",
    "    original_directory = os.path.expanduser(original_directory)\n",
    "    main(directory, original_directory)\n",
    "\n",
    "def combine_csv_files(result_path, output_file=None):\n",
    "    # Find all CSV files in the folder\n",
    "    csv_files = glob.glob(os.path.join(result_path, \"*dockq_fnat_scores.csv\"))\n",
    "    \n",
    "    # Read and combine all CSVs\n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        print(file)\n",
    "        # get filename without folder\n",
    "        filename = os.path.basename(file)\n",
    "        # remove the suffix\n",
    "        f = filename.replace(\"dockq_fnat_scores.csv\", \"\")\n",
    "    \n",
    "        # add new column\n",
    "        df[\"model_id\"] = f + df[\"model_id\"].astype(str)\n",
    "        df_list.append(df)\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Save to CSV if an output file is provided\n",
    "    if output_file:\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined CSV saved to {output_file}\")\n",
    "    return combined_df\n",
    "combined_df = combine_csv_files(result_path, dockq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a13b9-c84f-4191-bb08-a3b53557a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract ifplddt and pae and pdock2\n",
    "\n",
    "from Bio.PDB import PDBIO\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio.PDB.Selection import unfold_entities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys,os\n",
    "import argparse\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def retrieve_IFplddt(structure, chain1, chain2_lst, max_dist):\n",
    "    ## generate a dict to save IF_res_id\n",
    "    chain_lst = list(chain1) + chain2_lst\n",
    "\n",
    "    ifplddt = []\n",
    "    contact_chain_lst = []\n",
    "    for res1 in structure[0][chain1]:\n",
    "        for chain2 in chain2_lst:\n",
    "            count = 0\n",
    "            for res2 in structure[0][chain2]:\n",
    "\n",
    "                if res1.has_id('CA') and res2.has_id('CA'):\n",
    "                   dis = abs(res1['CA']-res2['CA'])\n",
    "                   ## add criteria to filter out disorder res\n",
    "                   if dis <= max_dist:\n",
    "                      ifplddt.append(res1['CA'].get_bfactor())\n",
    "                      count += 1\n",
    "\n",
    "                elif res1.has_id('CB') and res2.has_id('CB'):\n",
    "                   dis = abs(res1['CB']-res2['CB'])\n",
    "                   if dis <= max_dist:\n",
    "                      ifplddt.append(res1['CB'].get_bfactor())\n",
    "                      count += 1\n",
    "            if count > 0:\n",
    "              contact_chain_lst.append(chain2)\n",
    "    contact_chain_lst = sorted(list(set(contact_chain_lst)))   \n",
    "\n",
    "    if len(ifplddt)>0:\n",
    "       IF_plddt_avg = np.mean(ifplddt)\n",
    "    else:\n",
    "       IF_plddt_avg = 0\n",
    "    # print(IF_plddt_avg)\n",
    "\n",
    "    return IF_plddt_avg, contact_chain_lst\n",
    "\n",
    "\n",
    "def retrieve_IFPAEinter(structure, paeMat, contact_lst, max_dist):\n",
    "    ## contact_lst:the chain list that have an interface with each chain. For eg, a tetramer with A,B,C,D chains and A/B A/C B/D C/D interfaces,\n",
    "    ##             contact_lst would be [['B','C'],['A','D'],['A','D'],['B','C']]\n",
    "\n",
    "    chain_lst = [x.id for x in structure[0]]\n",
    "    seqlen = [len(x) for x in structure[0]]\n",
    "    ifch1_col=[]\n",
    "    ifch2_col=[]\n",
    "    ch1_lst=[]\n",
    "    ch2_lst=[]\n",
    "    ifpae_avg = []\n",
    "    d=10\n",
    "    for ch1_idx in range(len(chain_lst)):\n",
    "      ## extract x axis range from the PAE matrix\n",
    "      idx = chain_lst.index(chain_lst[ch1_idx])\n",
    "      ch1_sta=sum(seqlen[:idx])\n",
    "      ch1_end=ch1_sta+seqlen[idx]\n",
    "      ifpae_col = []   \n",
    "      ## for each chain that shares an interface with chain1, retrieve the PAE matrix for the specific part.\n",
    "      for contact_ch in contact_lst[ch1_idx]:\n",
    "        index = chain_lst.index(contact_ch)\n",
    "        ch_sta = sum(seqlen[:index])\n",
    "        ch_end = ch_sta+seqlen[index]\n",
    "        paeMat = np.array(paeMat)\n",
    "        remain_paeMatrix = paeMat[ch1_sta:ch1_end,ch_sta:ch_end]     \n",
    "\n",
    "        ## get avg PAE values for the interfaces for chain 1\n",
    "        mat_x = -1\n",
    "        for res1 in structure[0][chain_lst[ch1_idx]]:\n",
    "          mat_x += 1\n",
    "          mat_y = -1\n",
    "          for res2 in structure[0][contact_ch]:\n",
    "              mat_y+=1\n",
    "              if res1['CA'] - res2['CA'] <=max_dist:\n",
    "                 ifpae_col.append(remain_paeMatrix[mat_x,mat_y])\n",
    "      ## normalize by d(10A) first and then get the average\n",
    "      if not ifpae_col:\n",
    "        ifpae_avg.append(0)\n",
    "      else:\n",
    "        norm_if_interpae=np.mean(1/(1+(np.array(ifpae_col)/d)**2))\n",
    "        ifpae_avg.append(norm_if_interpae)\n",
    "        # print(ifpae_avg)\n",
    "    return ifpae_avg\n",
    "\n",
    "def calc_pmidockq(ifpae_norm, ifplddt):\n",
    "    df = pd.DataFrame()\n",
    "    df['ifpae_norm'] = ifpae_norm\n",
    "    df['ifplddt'] = ifplddt\n",
    "\n",
    "    df['prot'] = df.ifpae_norm*df.ifplddt\n",
    "    fitpopt = [1.31034849e+00, 8.47326239e+01, 7.47157696e-02, 5.01886443e-03] ## from orignal fit function  \n",
    "    df['pmidockq'] = sigmoid(df.prot.values, *fitpopt)\n",
    "    return df\n",
    "\n",
    "def sigmoid(x, L ,x0, k, b):\n",
    "    y = L / (1 + np.exp(-k*(x-x0)))+b\n",
    "    return (y)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio.PDB import PDBParser\n",
    "\n",
    "def process_pdb_file(pdb_file, npz_file, distance, file_id, chains_part=\"\"):\n",
    "    pdbp = PDBParser(QUIET=True)\n",
    "    structure = pdbp.get_structure('', pdb_file)\n",
    "    chains = [chain.id for chain in structure[0]]\n",
    "    remain_contact_lst = []\n",
    "    plddt_lst = []\n",
    "    # Assuming these functions are defined in the same file or imported\n",
    "    for idx in range(len(chains)):\n",
    "        chain2_lst = list(set(chains) - set(chains[idx]))\n",
    "        IF_plddt, contact_lst = retrieve_IFplddt(structure, chains[idx], chain2_lst, distance)\n",
    "        plddt_lst.append(IF_plddt)\n",
    "        remain_contact_lst.append(contact_lst)\n",
    "    # Load NPZ file instead of JSON\n",
    "    pae_data = np.load(npz_file)  \n",
    "    if \"pae\" not in pae_data:\n",
    "        raise KeyError(f\"'pae' key not found in {npz_file}\")\n",
    "    avgif_pae = retrieve_IFPAEinter(structure, pae_data[\"pae\"], remain_contact_lst, distance)  \n",
    "    res = calc_pmidockq(avgif_pae, plddt_lst)\n",
    "    pdb_id = os.path.basename(pdb_file).split('_')[0]\n",
    "    print(pdb_id)\n",
    "    \n",
    "    result = {\n",
    "        \"model_id\" : pdb_file,\n",
    "        \"pdb_id\": file_id,\n",
    "        \"pdb_id_with_chains\": '{0}_{1}'.format(pdb_id, chains_part),\n",
    "        \"ipae_norm_ag\": res['ifpae_norm'].tolist()[-1],\n",
    "        \"ipae_norm_avg\": np.mean(res['ifpae_norm']), \n",
    "        \"iplddt_ag\": res['ifplddt'].tolist()[-1],\n",
    "        \"iplddt_avg\": np.mean(res['ifplddt']), \n",
    "        \"pDockQ2_ag\": res['pmidockq'].tolist()[-1],\n",
    "        \"pDockQ2_avg\": np.mean(res['pmidockq'])}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23982755-956f-47ad-9cae-865becbc919c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def find_matching_npz(pdb_file, directory):\n",
    "    \"\"\"\n",
    "    Finds the matching NPZ file for a given PDB file based on the new naming format.\n",
    "    \"\"\"\n",
    "    pdb_file_basename = os.path.basename(pdb_file)\n",
    "\n",
    "    # Extract the model number (assuming last part before .pdb)\n",
    "    if pdb_file_basename.startswith(\"result_model_\") and pdb_file_basename.endswith(\".pdb\"):\n",
    "        model_number = pdb_file_basename.replace(\"result_model_\", \"\").replace(\".pdb\", \"\")\n",
    "    else:\n",
    "        print(f\"Skipping {pdb_file}: Filename format does not match expected pattern.\")\n",
    "        return None\n",
    "\n",
    "    # Construct expected NPZ filename\n",
    "    npz_filename = f\"pae_result_model_{model_number}.npz\"\n",
    "    npz_filepath = os.path.join(directory, npz_filename)\n",
    "\n",
    "    if os.path.exists(npz_filepath):\n",
    "        return npz_filepath\n",
    "    else:\n",
    "        print(f\"NPZ file not found for {pdb_file}: Expected {npz_filename}\")\n",
    "        return None\n",
    "\n",
    "def run_processing(directory):\n",
    "\n",
    "    pdb_id_wchains= directory.split(\"/\")[-3]\n",
    "    print(pdb_id_wchains)\n",
    "    \"\"\"\n",
    "    Finds matching PDB and NPZ files, processes them, and saves results.\n",
    "    \"\"\"\n",
    "    pdb_files = glob.glob(os.path.join(directory, \"result_model_*.pdb\"))\n",
    "    results = []\n",
    "\n",
    "    for pdb_file in pdb_files:\n",
    "        npz_file_name = find_matching_npz(pdb_file, directory)\n",
    "\n",
    "        if npz_file_name:\n",
    "            pdb_file_basename = os.path.basename(pdb_file)\n",
    "            model_number = pdb_file_basename.replace(\"result_model_\", \"\").replace(\"result_model_.pdb\", \"\")\n",
    "            file_id = f\"model_{model_number}\"\n",
    "            print(f\"Processing: {file_id}\")\n",
    "            # Process the matching PDB and NPZ files\n",
    "            result = process_pdb_file(pdb_file, npz_file_name, 8, file_id)\n",
    "            print(f\"Result for {pdb_file}: {result}\")\n",
    "            results.append(result)\n",
    "\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        os.chdir(result_path)\n",
    "        csv_file_path = str(pdb_id_wchains)+\"_pdockq2_fit.csv\"\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "        print(f\"Data saved to {csv_file_path}\")\n",
    "    else:\n",
    "        print(\"No results to save.\")\n",
    "\n",
    "### run for all predictions\n",
    "for i in complex_list:\n",
    "    directory = f\"{folder_path}/{i}/predictions/result\"\n",
    "    run_processing(directory)  \n",
    "\n",
    "def combine_csv_files(result_path, output_file=None):\n",
    "    csv_files = glob.glob(os.path.join(result_path, \"*_pdockq2_fit.csv\"))\n",
    "    df_list = [pd.read_csv(file) for file in csv_files]\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    if output_file:\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined CSV saved to {output_file}\")\n",
    "    return combined_df\n",
    "    \n",
    "combined_df = combine_csv_files(result_path, pdockq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb626e9-b024-4539-9c8e-e8a84925973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract boltz metrics\n",
    "x = []\n",
    "\n",
    "for i in complex_list:\n",
    "    df = pd.read_csv(f\"{folder_path}/{i}/metrics.csv\")\n",
    "    df['model_id'] = df['filename'].str.replace(\"confidence_result\",f\"{i}\")\n",
    "    df['model_id'] = df['model_id'].str.replace(\".json\",\"\")\n",
    "    df['model_id'] = df['model_id'].str.replace(\"model\",\"result_model\")\n",
    "    df=df[[ 'model_id','confidence_score', 'ptm', 'iptm', 'ligand_iptm', 'protein_iptm', 'complex_plddt', 'complex_pde', 'complex_ipde']]\n",
    "    x.append(df)\n",
    "dfy = pd.concat(x, axis=0)\n",
    "print(dfy.head())\n",
    "df = pd.read_csv(f\"{result_path}/{pdockq_output}\")\n",
    "df['model_id']=df['model_id'].str.replace(\"/predictions/result/\",\"_\").str.replace(\".pdb\",\"\")\n",
    "df['model_id']=df['model_id'].str.split(\"/\").str[-1]\n",
    "# print(df.head())\n",
    "df2 = pd.read_csv(f\"{result_path}/{dockq_output}\")\n",
    "df2['model_id']=df2['model_id'].str.replace(\"result\",\"boltz1_result\")\n",
    "# print(df2.head())\n",
    "dfx = pd.merge(df,df2,on=\"model_id\")\n",
    "\n",
    "df_fin = pd.merge(dfx,dfy,on=\"model_id\")\n",
    "df_fin=df_fin.drop_duplicates()\n",
    "# print(list(df_fin))\n",
    "df_fin[\"pdb_id\"]=df_fin[\"model_id\"].str.split(\"_\").str[0]\n",
    "df_fin[\"pdb_id_wchains\"]=df_fin[\"model_id\"].str.replace(\"_boltz\",\":\").str.split(\":\").str[0]\n",
    "df_fin=df_fin[['model_id', 'pdb_id', 'pdb_id_wchains', 'ipae_norm_ag', 'ipae_norm_avg', 'iplddt_ag', 'iplddt_avg', \n",
    "               'pDockQ2_ag', 'pDockQ2_avg', 'DockQ', 'fnat', 'iRMSD', 'LRMSD', 'F1', 'confidence_score', 'ptm', 'iptm', \n",
    "               'ligand_iptm', 'protein_iptm', 'complex_plddt', 'complex_pde', 'complex_ipde']]\n",
    "\n",
    "\n",
    "# AntiConf = 0.3pDockQ2_ag + 0.7pTM\n",
    "df_fin[\"AntiConf\"] =(0.3 * df_fin[\"pDockQ2_ag\"]) + (0.7 * df_fin[\"ptm\"])\n",
    "df_fin.to_csv(f\"{result_path}/{combined}\",index=False)\n",
    "df_fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
