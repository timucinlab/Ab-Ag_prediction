{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a956427f-b612-4b9d-9407-5efba9320429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pred_method = \"protenix\" ## folder containing predictions\n",
    "folder_path = f\"{pred_method}\"\n",
    "## file hierarchy should be like this\n",
    "# chai\n",
    "# ├── 7ar0_B_A_px\n",
    "# │   ├── 7ar0_B_A_px\n",
    "# │       ├── seed1\n",
    "# │           ├── predictions\n",
    "# │               ├── .pdb\n",
    "# │               ├── ...\n",
    "# ├── 7bnv_H_L_A_px\n",
    "# │   ├── 7bnv_H_L_A_px\n",
    "# │       ├── seed1\n",
    "# │           ├── predictions\n",
    "# │               ├── .pdb\n",
    "# │               ├── ...\n",
    "\n",
    "complex_list = [\n",
    "    f for f in os.listdir(folder_path)\n",
    "    if os.path.isdir(os.path.join(folder_path, f)) and not f.startswith(\".\")\n",
    "]\n",
    "print(complex_list)  # should print prediction folders ['7ar0_B_A_af2', '...', '...']\n",
    "\n",
    "original_directory = \"path/to/native_PDBs\" ## Native PDB directory\n",
    "\n",
    "## create results folder\n",
    "result_path = \"path/to/results_folder\"\n",
    "dockq_output = f\"{pred_method}_dockq_fnat_scores.csv\"  ## dockq output\n",
    "pdockq_output = f\"{pred_method}_pdockq2_fit.csv\" ## pdockq2 output\n",
    "combined = f\"{pred_method}_combined.csv\" ### combined results with dockq_pdock2 and model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f772cf30-dbdd-4faf-845b-ddad61921bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### cif conversion\n",
    "import pandas as pd\n",
    "from Bio import PDB\n",
    "import os\n",
    "\n",
    "# Custom MMCIFParser to handle missing B-factors, occupancy, and renaming chains\n",
    "class CustomMMCIFParser(PDB.MMCIFParser):\n",
    "    def _build_structure(self, structure_id):\n",
    "        mmcif_dict = self._mmcif_dict\n",
    "        try:\n",
    "            # If B-factors are missing, create a default list of zeros\n",
    "            b_factor_list = mmcif_dict[\"_atom_site.B_iso_or_equiv\"]\n",
    "        except KeyError:\n",
    "            atom_count = len(mmcif_dict[\"_atom_site.group_PDB\"])\n",
    "            b_factor_list = [\"0.0\"] * atom_count\n",
    "            mmcif_dict[\"_atom_site.B_iso_or_equiv\"] = b_factor_list\n",
    "        \n",
    "        try:\n",
    "            # If occupancy is missing, create a default list of zeros\n",
    "            occupancy_list = mmcif_dict[\"_atom_site.occupancy\"]\n",
    "        except KeyError:\n",
    "            atom_count = len(mmcif_dict[\"_atom_site.group_PDB\"])\n",
    "            occupancy_list = [\"0.0\"] * atom_count\n",
    "            mmcif_dict[\"_atom_site.occupancy\"] = occupancy_list\n",
    "        \n",
    "        super()._build_structure(structure_id)\n",
    "\n",
    "# Function to rename chain IDs\n",
    "def rename_chains(structure):\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            chain.id = chain.id.replace('0', '')  # Replace '0' in chain ID (e.g., A0 → A)\n",
    "    return structure\n",
    "\n",
    "for f in complex_list:\n",
    "    wdir = f'{folder_path}/{f}/{f}/seed_1/predictions/'\n",
    "    \n",
    "    # Initialize the custom parser and writer\n",
    "    parser = CustomMMCIFParser(QUIET=True)  #  reading .cif files\n",
    "    writer = PDB.PDBIO()                   #  writing .pdb files\n",
    "    \n",
    "    # Loop through all .cif files in the directory\n",
    "    for file_name in os.listdir(wdir):\n",
    "        if file_name.endswith(\".cif\"):\n",
    "            input_path = os.path.join(wdir, file_name)\n",
    "            output_path = os.path.join(wdir, file_name.replace(\".cif\", \".pdb\"))\n",
    "            \n",
    "            try:\n",
    "                # Parse the .cif file\n",
    "                structure = parser.get_structure(file_name, input_path)\n",
    "                \n",
    "                # Rename chain IDs\n",
    "                structure = rename_chains(structure)\n",
    "                \n",
    "                # Write to .pdb format\n",
    "                writer.set_structure(structure)\n",
    "                writer.save(output_path)\n",
    "                print(f\"Converted {file_name} to {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "    \n",
    "    print(f\"Conversion of {f} is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758f617-7ff4-457a-ab35-4072ef9237bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "from DockQ.DockQ import load_PDB, run_on_all_native_interfaces\n",
    "from statistics import mean\n",
    "\n",
    "def merge_chains(model, chains_to_merge):\n",
    "    \"\"\"\n",
    "    Merges specified chains in the given model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Bio.PDB.Structure\n",
    "        The model in which the chains are to be merged.\n",
    "    chains_to_merge : list of str\n",
    "        The list of chain IDs to be merged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : Bio.PDB.Structure\n",
    "        The model with the specified chains merged.\n",
    "    \"\"\"\n",
    "    print(f\"Merging chains {chains_to_merge} in model\")\n",
    "    for chain in chains_to_merge[1:]:\n",
    "        for res in list(model[chain]):\n",
    "            res.id = (chains_to_merge[0], res.id[1], res.id[2])\n",
    "            model[chains_to_merge[0]].add(res)\n",
    "        model.detach_child(chain)\n",
    "    model[chains_to_merge[0]].id = \"\".join(chains_to_merge)\n",
    "    return model\n",
    "\n",
    "def calculate_dockq(model, native, chain_map):\n",
    "    \"\"\"\n",
    "    Calculates DockQ scores for the given model and native structures based on the chain map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Bio.PDB.Structure\n",
    "        The model structure.\n",
    "    native : Bio.PDB.Structure\n",
    "        The native structure.\n",
    "    chain_map : dict\n",
    "        The mapping of chains between model and native structures.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        The results containing various DockQ metrics.\n",
    "    dockq_score : float\n",
    "        The DockQ score.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating DockQ score with chain_map: {chain_map}\")\n",
    "    results, dockq_score = run_on_all_native_interfaces(model, native, chain_map=chain_map)\n",
    "    return results, dockq_score\n",
    "\n",
    "def process_models(models):\n",
    "    \"\"\"\n",
    "    Processes the provided models and calculates DockQ scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list of tuple\n",
    "        List of tuples where each tuple contains the model file path and native file path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_list : list of tuple\n",
    "        List of tuples containing model_id, DockQ, DockQ_F1, and fnat scores.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    for model_file, native_file in models:\n",
    "        print(f\"Processing model: {model_file}, native: {native_file}\")\n",
    "        model_id = os.path.basename(model_file).split(\".\")[0]\n",
    "        model = load_PDB(model_file)\n",
    "        native = load_PDB(native_file)\n",
    "\n",
    "        chain_ids = list(model.child_dict.keys())\n",
    "        print(chain_ids)\n",
    "        native_chain_ids = list(native.child_dict.keys())\n",
    "\n",
    "        if len(chain_ids) == 3:\n",
    "            print(f\"Model {model_id} has 3 chains: {chain_ids}\")\n",
    "            \n",
    "            # Merge A and B chains and recalculate\n",
    "            model_merged = merge_chains(model, chain_ids[:2])\n",
    "            print(model_merged)\n",
    "            native_merged = merge_chains(native, native_chain_ids[:2])\n",
    "            chain_map_merged = {native_chain_ids[2]: chain_ids[2], \"\".join(native_chain_ids[:2]): \"\".join(chain_ids[:2])}\n",
    "            results_merged, dockq_score_merged = calculate_dockq(model_merged, native_merged, chain_map_merged)\n",
    "            merged_result = results_merged[list(results_merged.keys())[0]]\n",
    "            # results_list.append((model_id, merged_result['DockQ'], merged_result['DockQ_F1'], merged_result['fnat']))\n",
    "            results_list.append((model_id, merged_result['DockQ'], merged_result['fnat'],\n",
    "                                merged_result['iRMSD'], merged_result['LRMSD'],merged_result['F1']))\n",
    "\n",
    "        elif len(chain_ids) == 2:\n",
    "            print(f\"Model {model_id} has 2 chains: {chain_ids}\")\n",
    "            # Assume there are only 2 chains\n",
    "            chain_map = {native_chain_ids[0]: chain_ids[0], native_chain_ids[1]: chain_ids[1]}\n",
    "            results, dockq_score = calculate_dockq(model, native, chain_map)\n",
    "            results_list.append((model_id, results[list(results.keys())[0]]['DockQ'], results[list(results.keys())[0]]['fnat'],\n",
    "                                results[list(results.keys())[0]]['iRMSD'], results[list(results.keys())[0]]['LRMSD'],\n",
    "                                results[list(results.keys())[0]]['F1']))\n",
    "\n",
    "        else:\n",
    "            print(f\"Model {model_id} does not have 2 or 3 chains: {chain_ids}, skipping.\")\n",
    "            continue\n",
    "\n",
    "    return results_list\n",
    "\n",
    "def save_results_to_csv(results, filename):\n",
    "    print(f\"Saving results to CSV file: {filename}\")\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # writer.writerow(['model_id', 'DockQ', 'DockQ_F1', 'fnat'])\n",
    "        writer.writerow(['model_id', 'DockQ', 'fnat',\"iRMSD\",\"LRMSD\",\"F1\"])\n",
    "\n",
    "        for model_id, dockq, fnat, irms, lrms, f1 in results:\n",
    "            # print(f\"Writing row: {model_id}, {dockq}, {dockq_f1}, {fnat}\")\n",
    "            # writer.writerow([model_id, dockq, dockq_f1, fnat])\n",
    "            print(f\"Writing row: {model_id}, {dockq}, {fnat}, {irms},{lrms},{f1}\")\n",
    "            writer.writerow([model_id, dockq, fnat, irms, lrms,f1])\n",
    "\n",
    "def main(directory, original_directory):\n",
    "    \"\"\"\n",
    "    Main function to find PDB files, process models, and save results to a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        Path to the directory containing PDB files to be analyzed.\n",
    "    original_directory : str\n",
    "        Path to the directory containing original PDB files for comparison.\n",
    "\n",
    "    Returns\n",
    "    -------                                                                                                                                                                                                                                                                                                                                    \n",
    "    None\n",
    "    \"\"\"\n",
    "    pdb_files = glob.glob(f\"{directory}/*_seed_1*.pdb\")\n",
    "    if not pdb_files:\n",
    "        print(f\"No PDB files found in directory: {directory}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found PDB files: {pdb_files}\")\n",
    "    models = []\n",
    "\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_id_chains = os.path.basename(pdb_file).replace(\"_px\",\":\").split(':')[0] \n",
    "        print(f\"Searching for original files for: {pdb_id_chains}\")\n",
    "        original_pdb_files = glob.glob(f\"{original_directory}/{pdb_id_chains}.pdb\")  # Find matching original PDB files\n",
    "        print(f\"Found original PDB files: {original_pdb_files} for {pdb_file}\")\n",
    "        if not original_pdb_files:\n",
    "            print(f\"No matching original PDB file found for {pdb_file}, skipping.\")\n",
    "            continue\n",
    "        for original_pdb_file in original_pdb_files:\n",
    "            models.append((pdb_file, original_pdb_file))\n",
    "            print(f\"Adding model-native pair: {pdb_file}, {original_pdb_file}\")\n",
    "\n",
    "    if not models:\n",
    "        print(\"No valid model-native pairs found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    os.chdir(result_path)\n",
    "    results = process_models(models)\n",
    "    save_results_to_csv(results, str(pdb_id_chains)+'_dockq_fnat_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32ffba-9ec2-4947-9a9c-a4542b881fe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### dockq and fnat calculation running\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "for i in complex_list:\n",
    "    fpath = f\"{folder_path}/{i}/{i}/seed_1/predictions/\"\n",
    "    directory = os.path.expanduser(fpath)\n",
    "    original_directory = os.path.expanduser(original_directory)\n",
    "    main(directory, original_directory)\n",
    "    \n",
    "    def combine_csv_files(result_path, output_file=None):\n",
    "        # Find all CSV files in the folder\n",
    "        csv_files = glob.glob(os.path.join(result_path, \"*dockq_fnat_scores.csv\"))\n",
    "        \n",
    "        # Read and combine all CSVs\n",
    "        df_list = [pd.read_csv(file) for file in csv_files]\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "        # Save to CSV if an output file is provided\n",
    "        if output_file:\n",
    "            combined_df.to_csv(output_file, index=False)\n",
    "            print(f\"Combined CSV saved to {output_file}\")\n",
    "        return combined_df\n",
    "combined_df = combine_csv_files(result_path, dockq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e315cef-9aa4-4c90-9803-8e7986c89335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### plddt extraction\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_sample_number(filename):\n",
    "    \"\"\"Extract the last number from the filename.\"\"\"\n",
    "    match = re.search(r'(\\d+)$', filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def update_pdb_with_plddt(folder_path):\n",
    "    pdb_files = glob.glob(os.path.join(folder_path, \"*.pdb\"))\n",
    "\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_basename = os.path.basename(pdb_file).replace(\".pdb\", \"\")\n",
    "        sample_number = extract_sample_number(pdb_basename)\n",
    "\n",
    "        if not sample_number:\n",
    "            print(f\"⚠No sample number found in {pdb_basename}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Locate the corresponding JSON file\n",
    "        json_pattern = os.path.join(folder_path, f\"*full_data_sample_{sample_number}.json\")\n",
    "        json_files = glob.glob(json_pattern)\n",
    "\n",
    "        if not json_files:\n",
    "            print(f\"No matching JSON found for {pdb_basename}\")\n",
    "            continue\n",
    "\n",
    "        json_file = json_files[0]  \n",
    "\n",
    "        # Load atom_plddt values\n",
    "        with open(json_file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        plddt_values = data.get(\"atom_plddt\", [])\n",
    "        if not plddt_values:\n",
    "            print(f\"⚠No atom_plddt found in {json_file}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Read PDB file and replace B-factor column\n",
    "        pdb_lines = []\n",
    "        plddt_index = 0\n",
    "\n",
    "        with open(pdb_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith((\"ATOM\", \"HETATM\")):\n",
    "                    if plddt_index < len(plddt_values):\n",
    "                        # Replace B-factor (columns 61-66) with pLDDT value\n",
    "                        new_line = f\"{line[:60]}{plddt_values[plddt_index]:6.2f}{line[66:]}\"\n",
    "                        pdb_lines.append(new_line)\n",
    "                        plddt_index += 1\n",
    "                    else:\n",
    "                        pdb_lines.append(line)\n",
    "                else:\n",
    "                    pdb_lines.append(line)\n",
    "\n",
    "        # Overwrite the original PDB file\n",
    "        with open(pdb_file, \"w\") as f:\n",
    "            f.writelines(pdb_lines)\n",
    "\n",
    "        print(f\"Updated {pdb_basename}.pdb with pLDDT values from {os.path.basename(json_file)}.\")\n",
    "\n",
    "for i in complex_list:\n",
    "    f = f\"{folder_path}/{i}/{i}/seed_1/predictions/\"\n",
    "    update_pdb_with_plddt(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6b612-4949-4eff-8574-dc0abbc939db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from Bio.PDB import PDBIO\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio.PDB.Selection import unfold_entities\n",
    "\n",
    "import numpy as np\n",
    "import sys,os\n",
    "import argparse\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def retrieve_IFplddt(structure, chain1, chain2_lst, max_dist):\n",
    "    ## generate a dict to save IF_res_id\n",
    "    chain_lst = list(chain1) + chain2_lst\n",
    "\n",
    "    ifplddt = []\n",
    "    contact_chain_lst = []\n",
    "    for res1 in structure[0][chain1]:\n",
    "        for chain2 in chain2_lst:\n",
    "            count = 0\n",
    "            for res2 in structure[0][chain2]:\n",
    "\n",
    "                if res1.has_id('CA') and res2.has_id('CA'):\n",
    "                   dis = abs(res1['CA']-res2['CA'])\n",
    "                   ## add criteria to filter out disorder res\n",
    "                   if dis <= max_dist:\n",
    "                      ifplddt.append(res1['CA'].get_bfactor()*100)\n",
    "                      count += 1\n",
    "\n",
    "                elif res1.has_id('CB') and res2.has_id('CB'):\n",
    "                   dis = abs(res1['CB']-res2['CB'])\n",
    "                   if dis <= max_dist:\n",
    "                      ifplddt.append(res1['CB'].get_bfactor()*100)\n",
    "                      count += 1\n",
    "            if count > 0:\n",
    "              contact_chain_lst.append(chain2)\n",
    "    contact_chain_lst = sorted(list(set(contact_chain_lst)))   \n",
    "\n",
    "    if len(ifplddt)>0:\n",
    "       IF_plddt_avg = np.mean(ifplddt)\n",
    "    else:\n",
    "       IF_plddt_avg = 0\n",
    "    return IF_plddt_avg, contact_chain_lst\n",
    "\n",
    "\n",
    "def retrieve_IFPAEinter(structure, paeMat, contact_lst, max_dist):\n",
    "    ## contact_lst:the chain list that have an interface with each chain. For eg, a tetramer with A,B,C,D chains and A/B A/C B/D C/D interfaces,\n",
    "    ##             contact_lst would be [['B','C'],['A','D'],['A','D'],['B','C']]\n",
    "\n",
    "    chain_lst = [x.id for x in structure[0]]\n",
    "    seqlen = [len(x) for x in structure[0]]\n",
    "    ifch1_col=[]\n",
    "    ifch2_col=[]\n",
    "    ch1_lst=[]\n",
    "    ch2_lst=[]\n",
    "    ifpae_avg = []\n",
    "    d=10\n",
    "    for ch1_idx in range(len(chain_lst)):\n",
    "      ## extract x axis range from the PAE matrix\n",
    "      idx = chain_lst.index(chain_lst[ch1_idx])\n",
    "      ch1_sta=sum(seqlen[:idx])\n",
    "      ch1_end=ch1_sta+seqlen[idx]\n",
    "      ifpae_col = []   \n",
    "      ## for each chain that shares an interface with chain1, retrieve the PAE matrix for the specific part.\n",
    "      for contact_ch in contact_lst[ch1_idx]:\n",
    "        index = chain_lst.index(contact_ch)\n",
    "        ch_sta = sum(seqlen[:index])\n",
    "        ch_end = ch_sta+seqlen[index]\n",
    "        paeMat = np.array(paeMat)\n",
    "        remain_paeMatrix = paeMat[ch1_sta:ch1_end,ch_sta:ch_end]   \n",
    "\n",
    "        ## get avg PAE values for the interfaces for chain 1\n",
    "        mat_x = -1\n",
    "        for res1 in structure[0][chain_lst[ch1_idx]]:\n",
    "          mat_x += 1\n",
    "          mat_y = -1\n",
    "          for res2 in structure[0][contact_ch]:\n",
    "              mat_y+=1\n",
    "              if res1['CA'] - res2['CA'] <=max_dist:\n",
    "                 ifpae_col.append(remain_paeMatrix[mat_x,mat_y])\n",
    "      ## normalize by d(10A) first and then get the average\n",
    "      if not ifpae_col:\n",
    "        ifpae_avg.append(0)\n",
    "      else:\n",
    "        norm_if_interpae=np.mean(1/(1+(np.array(ifpae_col)/d)**2))\n",
    "        ifpae_avg.append(norm_if_interpae)\n",
    "        # print(ifpae_avg)\n",
    "    return ifpae_avg\n",
    "\n",
    "def calc_pmidockq(ifpae_norm, ifplddt):\n",
    "    df = pd.DataFrame()\n",
    "    df['ifpae_norm'] = ifpae_norm\n",
    "    df['ifplddt'] = ifplddt\n",
    "\n",
    "    df['prot'] = df.ifpae_norm*df.ifplddt\n",
    "    fitpopt = [1.31034849e+00, 8.47326239e+01, 7.47157696e-02, 5.01886443e-03] ## from orignal fit function  \n",
    "    df['pmidockq'] = sigmoid(df.prot.values, *fitpopt)\n",
    "    return df\n",
    "\n",
    "def sigmoid(x, L ,x0, k, b):\n",
    "    y = L / (1 + np.exp(-k*(x-x0)))+b\n",
    "    return (y)\n",
    "\n",
    "def find_matching_json(pdb_file, directory):\n",
    "    \"\"\"Find the corresponding JSON file by matching the last number in the filename.\"\"\"\n",
    "    pdb_file_basename = os.path.basename(pdb_file)\n",
    "    \n",
    "    last_digit = ''.join([char for char in pdb_file_basename if char.isdigit()])[-2:]  \n",
    "    \n",
    "    json_pattern = f\"*full_data_sample_{last_digit}.json\"  \n",
    "    json_files = glob.glob(os.path.join(directory, json_pattern))\n",
    "\n",
    "    return json_files[0] if json_files else None \n",
    "\n",
    "def process_pdb_file(pdb_file, json_file, distance, file_id, chains_part=\"\"):\n",
    "    \"\"\"Process PDB and JSON files to compute interface metrics.\"\"\"\n",
    "    pdbp = PDBParser(QUIET=True)\n",
    "    structure = pdbp.get_structure('', pdb_file)\n",
    "    chains = [chain.id for chain in structure[0]]\n",
    "\n",
    "    remain_contact_lst = []\n",
    "    plddt_lst = []\n",
    "\n",
    "    for idx in range(len(chains)):\n",
    "        chain2_lst = list(set(chains) - {chains[idx]})\n",
    "        IF_plddt, contact_lst = retrieve_IFplddt(structure, chains[idx], chain2_lst, distance)\n",
    "        plddt_lst.append(IF_plddt)\n",
    "        remain_contact_lst.append(contact_lst)\n",
    "\n",
    "    pae_data = pd.read_json(json_file, lines=True)\n",
    "    if \"token_pair_pae\" in pae_data:\n",
    "        pae_matrix = pae_data[\"token_pair_pae\"][0]  # Extract PAE matrix\n",
    "    else:\n",
    "        print(f\"Warning: 'token_pair_pae' not found in {json_file}\")\n",
    "        return None  \n",
    "\n",
    "    avgif_pae = retrieve_IFPAEinter(structure, pae_matrix, remain_contact_lst, distance)\n",
    "    res = calc_pmidockq(avgif_pae, plddt_lst)\n",
    "\n",
    "    pdb_id = os.path.basename(pdb_file).split('_')[0]\n",
    "    result = {\n",
    "        \"model_id\" : pdb_file,\n",
    "        \"pdb_id\": file_id,\n",
    "        \"pdb_id_with_chains\": '{0}_{1}'.format(pdb_id, chains_part),\n",
    "        \"ipae_norm_ag\": res['ifpae_norm'].tolist()[-1],\n",
    "        \"ipae_norm_avg\": np.mean(res['ifpae_norm']), \n",
    "        \"iplddt_ag\": res['ifplddt'].tolist()[-1],\n",
    "        \"iplddt_avg\": np.mean(res['ifplddt']), \n",
    "        \"pDockQ2_ag\": res['pmidockq'].tolist()[-1],\n",
    "        \"pDockQ2_avg\": np.mean(res['pmidockq'])}\n",
    "    return result\n",
    "\n",
    "def run_processing(directory):\n",
    "    \"\"\"Process all PDB files in the directory and match with JSON files.\"\"\"\n",
    "    pdb_files = glob.glob(os.path.join(directory, \"*_px_seed_1*.pdb\"))\n",
    "    results = []\n",
    "\n",
    "    for pdb_file in pdb_files:\n",
    "        json_file_name = find_matching_json(pdb_file, directory)\n",
    "\n",
    "        if json_file_name and os.path.exists(json_file_name):\n",
    "            base_name = pdb_file.rsplit('_', 4)[0]\n",
    "            pdb_file_basename = os.path.basename(pdb_file)\n",
    "            parts = pdb_file_basename.split('_')\n",
    "            # print(f\"Filena/me parts: {parts}\")\n",
    "\n",
    "            pdb_id = parts[0]\n",
    "            chains_part = \"_\".join(parts[1:parts.index('px')])\n",
    "            print(f\"Processing: {pdb_id}_{chains_part}\")\n",
    "            file_id = f\"{pdb_id}_{chains_part}\"\n",
    "            result = process_pdb_file(pdb_file, json_file_name, 8, pdb_id, chains_part)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        else:\n",
    "            print(f\"No matching JSON file found for {pdb_file}\")\n",
    "\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        csv_file_path = f\"{file_id}_pdockq2_fit.csv\"\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "        print(f\"Data saved to {csv_file_path}\")\n",
    "    else:\n",
    "        print(\"No results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29dc27e-14a6-41cb-a528-bf90d313b0a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "# pdockq2 functions \n",
    "for i in complex_list:\n",
    "    directory = f\"{folder_path}/{i}/{i}/seed_1/predictions/\"\n",
    "    run_processing(directory)   \n",
    "\n",
    "def combine_csv_files(result_path, output_file=None):\n",
    "    csv_files = glob.glob(os.path.join(result_path, \"*_pdockq2_fit.csv\"))\n",
    "    df_list = [pd.read_csv(file) for file in csv_files]\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    if output_file:\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined CSV saved to {output_file}\")\n",
    "    return combined_df\n",
    "    \n",
    "combined_df = combine_csv_files(result_path, pdockq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2120ea-929d-4f22-96f2-5127c14735ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## proteinx metrics\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def extract_data_from_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return {\n",
    "        \"filename\": os.path.basename(file_path),\n",
    "        \"plddt\": data.get(\"plddt\"),\n",
    "        \"gpde\": data.get(\"gpde\"),\n",
    "        \"ptm\": data.get(\"ptm\"),\n",
    "        \"iptm\": data.get(\"iptm\"),\n",
    "    }\n",
    "\n",
    "x=[]\n",
    "for i in complex_list:\n",
    "    for f in range(25):\n",
    "        file_path = f\"{folder_path}/{i}/{i}/seed_1/predictions/{i}_seed_1_summary_confidence_sample_{f}.json\"\n",
    "        data = extract_data_from_json(file_path)\n",
    "        df = pd.DataFrame([data])\n",
    "        x.append(df)\n",
    "\n",
    "dfy = pd.concat(x)\n",
    "# print(list(dfy))\n",
    "dfy['model_id'] = dfy['filename'].str.replace('_summary_confidence', \"\").str.replace('.json', \"\")\n",
    "dfy=dfy[['model_id','plddt', 'gpde', 'ptm', 'iptm']]\n",
    "\n",
    "df = pd.read_csv(f\"{result_path}/{pdockq_output}\")\n",
    "df['model_id']=df['model_id'].str.replace(\".pdb\",\"\")\n",
    "df['model_id']=df['model_id'].str.split(\"/\").str[-1]\n",
    "# print(df.head())\n",
    "\n",
    "df2 = pd.read_csv(f\"{result_path}/{dockq_output}\")\n",
    "df2['model_id']=df2['model_id'].str.replace(\".pdb\",\"\")\n",
    "# print(df2.head())\n",
    "dfx = pd.merge(df,df2,on=\"model_id\")\n",
    "\n",
    "df_fin = pd.merge(dfx,dfy,on=\"model_id\")\n",
    "df_fin=df_fin.drop_duplicates()\n",
    "\n",
    "# AntiConf = 0.3pDockQ2_ag + 0.7pTM\n",
    "df_fin[\"AntiConf\"] =(0.3 * df_fin[\"pDockQ2_ag\"]) + (0.7 * df_fin[\"ptm\"])\n",
    "df_fin.to_csv(f\"{result_path}/{combined}\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
