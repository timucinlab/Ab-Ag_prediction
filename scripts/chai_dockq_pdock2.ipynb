{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d6820-fb60-4365-851e-cf166ec63b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pred_method = \"chai\" ## folder containing predictions\n",
    "folder_path = f\"{pred_method}\"\n",
    "## file hierarchy should be like this\n",
    "# chai\n",
    "# ├── 7ar0_B_A_chai\n",
    "# │   ├── xxx.pdb\n",
    "# │   ├── ...\n",
    "# ├── 7bnv_H_L_A_chai\n",
    "# │   ├── xxx.pdb\n",
    "# │   ├── ...\n",
    "\n",
    "complex_list = [\n",
    "    f for f in os.listdir(folder_path)\n",
    "    if os.path.isdir(os.path.join(folder_path, f)) and not f.startswith(\".\")\n",
    "]\n",
    "print(complex_list)  # should print prediction folders ['7ar0_B_A_af2', '...', '...']\n",
    "\n",
    "original_directory = \"path/to/native_PDBs\" ## Native PDB directory\n",
    "\n",
    "## create results folder\n",
    "result_path = \"path/to/results_folder\"\n",
    "dockq_output = f\"{pred_method}_dockq_fnat_scores.csv\"  ## dockq output\n",
    "pdockq_output = f\"{pred_method}_pdockq2_fit.csv\" ## pdockq2 output\n",
    "combined = f\"{pred_method}_combined.csv\" ### combined results with dockq_pdock2 and model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c64800-87e4-4d98-95c5-b3d2aeae8d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### cif conversion\n",
    "import pandas as pd\n",
    "from Bio import PDB\n",
    "import os\n",
    "import os\n",
    "from Bio import PDB\n",
    "\n",
    "parser = PDB.MMCIFParser(QUIET=True)\n",
    "writer = PDB.PDBIO()\n",
    "\n",
    "for f in complex_list:\n",
    "    fpath = os.path.join(folder_path, f)  # full dir path\n",
    "    print(\"Scanning:\", fpath)\n",
    "    if not os.path.isdir(fpath):\n",
    "        print(\"! Skipping, not a directory:\", fpath)\n",
    "        continue\n",
    "\n",
    "    for file_name in os.listdir(fpath):\n",
    "        if file_name.lower().endswith(\".cif\"):\n",
    "            input_path = os.path.join(fpath, file_name)             \n",
    "            output_path = os.path.join(\n",
    "                fpath, os.path.splitext(file_name)[0] + \".pdb\"\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(input_path):\n",
    "                print(\"! Not found:\", input_path)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                structure = parser.get_structure(file_name, input_path)\n",
    "                writer.set_structure(structure)\n",
    "                writer.save(output_path)\n",
    "                print(f\"Converted {file_name} -> {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"! Failed on {input_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c596a960-c648-4f1c-b646-8b4290c21342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## dckq and pdock2 Functions\n",
    "\n",
    "### rename for unrelaxed\n",
    "import os\n",
    "for i in complex_list:\n",
    "    print(i)\n",
    "    fpath = f\"{folder_path}/{i}/\"\n",
    "    print(fpath)\n",
    "    prefix = str(i)+\"_unrelaxed\"  # prefix from af2\n",
    "    for filename in os.listdir(fpath):\n",
    "        if filename.endswith(\".pdb\"):\n",
    "            old_path = os.path.join(fpath, filename)\n",
    "            new_filename = prefix + filename\n",
    "            new_path = os.path.join(fpath, new_filename)\n",
    "    \n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Renamed: {filename} → {new_filename}\")\n",
    "    \n",
    "    print(\"Renaming complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f01b5e-fda8-436c-94d7-78169e9dd97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dockq Functions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "from DockQ.DockQ import load_PDB, run_on_all_native_interfaces\n",
    "from statistics import mean\n",
    "\n",
    "def merge_chains(model, chains_to_merge):\n",
    "    print(f\"Merging chains {chains_to_merge} in model\")\n",
    "    for chain in chains_to_merge[1:]:\n",
    "        for res in list(model[chain]):\n",
    "            res.id = (chains_to_merge[0], res.id[1], res.id[2])\n",
    "            model[chains_to_merge[0]].add(res)\n",
    "        model.detach_child(chain)\n",
    "    model[chains_to_merge[0]].id = \"\".join(chains_to_merge)\n",
    "    return model\n",
    "\n",
    "def calculate_dockq(model, native, chain_map):\n",
    "    print(f\"Calculating DockQ score with chain_map: {chain_map}\")\n",
    "    results, dockq_score = run_on_all_native_interfaces(model, native, chain_map=chain_map)\n",
    "    return results, dockq_score\n",
    "\n",
    "def process_models(models):\n",
    "    results_list = []\n",
    "    for model_file, native_file in models:\n",
    "        print(f\"Processing model: {model_file}, native: {native_file}\")\n",
    "        model_id = os.path.basename(model_file).split(\".\")[0]\n",
    "        model_id = os.path.basename(model_file)\n",
    "        model = load_PDB(model_file)\n",
    "        native = load_PDB(native_file)\n",
    "\n",
    "        chain_ids = list(model.child_dict.keys())\n",
    "        print(chain_ids)\n",
    "        native_chain_ids = list(native.child_dict.keys())\n",
    "\n",
    "        if len(chain_ids) == 3:\n",
    "            print(f\"Model {model_id} has 3 chains: {chain_ids}\")\n",
    "            \n",
    "            # Merge A and B chains and recalculate\n",
    "            model_merged = merge_chains(model, chain_ids[:2])\n",
    "            print(model_merged)\n",
    "            native_merged = merge_chains(native, native_chain_ids[:2])\n",
    "            chain_map_merged = {native_chain_ids[2]: chain_ids[2], \"\".join(native_chain_ids[:2]): \"\".join(chain_ids[:2])}\n",
    "            results_merged, dockq_score_merged = calculate_dockq(model_merged, native_merged, chain_map_merged)\n",
    "            merged_result = results_merged[list(results_merged.keys())[0]]\n",
    "            # results_list.append((model_id, merged_result['DockQ'], merged_result['DockQ_F1'], merged_result['fnat']))\n",
    "            results_list.append((model_id, merged_result['DockQ'], merged_result['fnat'],\n",
    "                                merged_result['iRMSD'], merged_result['LRMSD'],merged_result['F1']))\n",
    "\n",
    "        elif len(chain_ids) == 2:\n",
    "            print(f\"Model {model_id} has 2 chains: {chain_ids}\")\n",
    "            # Assume there are only 2 chains\n",
    "            chain_map = {native_chain_ids[0]: chain_ids[0], native_chain_ids[1]: chain_ids[1]}\n",
    "            results, dockq_score = calculate_dockq(model, native, chain_map)\n",
    "            results_list.append((model_id, results[list(results.keys())[0]]['DockQ'], results[list(results.keys())[0]]['fnat'],\n",
    "                                results[list(results.keys())[0]]['iRMSD'], results[list(results.keys())[0]]['LRMSD'],\n",
    "                                results[list(results.keys())[0]]['F1']))\n",
    "\n",
    "        else:\n",
    "            print(f\"Model {model_id} does not have 2 or 3 chains: {chain_ids}, skipping.\")\n",
    "            continue\n",
    "\n",
    "    return results_list\n",
    "\n",
    "def save_results_to_csv(results, filename):\n",
    "    print(f\"Saving results to CSV file: {filename}\")\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # writer.writerow(['model_id', 'DockQ', 'DockQ_F1', 'fnat'])\n",
    "        writer.writerow(['model_id', 'DockQ', 'fnat',\"iRMSD\",\"LRMSD\",\"F1\"])\n",
    "\n",
    "        for model_id, dockq, fnat, irms, lrms, f1 in results:\n",
    "            # print(f\"Writing row: {model_id}, {dockq}, {dockq_f1}, {fnat}\")\n",
    "            # writer.writerow([model_id, dockq, dockq_f1, fnat])\n",
    "            print(f\"Writing row: {model_id}, {dockq}, {fnat}, {irms},{lrms},{f1}\")\n",
    "            writer.writerow([model_id, dockq, fnat, irms, lrms,f1])  \n",
    "\n",
    "def main(directory, original_directory):\n",
    "    pdb_files = glob.glob(f\"{directory}/*_unrelaxed*.pdb\")\n",
    "    if not pdb_files:\n",
    "        print(f\"No PDB files found in directory: {directory}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found PDB files: {pdb_files}\")\n",
    "    models = []\n",
    "\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_id_chains = os.path.basename(directory)\n",
    "        pdb_id_chains= directory.split(\"/\")[-1].replace(\"_chai\",\"\")\n",
    "        print(pdb_id_chains)\n",
    "        # print(pdb_id_chains)\n",
    "        # pdb_id_chains = \"_\".join(pdb_id_chains.split(\"_\")[:-1])\n",
    "        print(f\"Searching for original files for: {pdb_id_chains}\")\n",
    "        original_pdb_files = glob.glob(f\"{original_directory}/{pdb_id_chains}.pdb\")  # Find matching original PDB files\n",
    "        print(f\"Found original PDB files: {original_pdb_files} for {pdb_file}\")\n",
    "        if not original_pdb_files:\n",
    "            print(f\"No matching original PDB file found for {pdb_file}, skipping.\")\n",
    "            continue\n",
    "        for original_pdb_file in original_pdb_files:\n",
    "            models.append((pdb_file, original_pdb_file))\n",
    "            print(f\"Adding model-native pair: {pdb_file}, {original_pdb_file}\")\n",
    "\n",
    "    if not models:\n",
    "        print(\"No valid model-native pairs found. Exiting.\")\n",
    "        return\n",
    "    os.chdir(result_path)\n",
    "    results = process_models(models)\n",
    "    save_results_to_csv(results, str(pdb_id_chains)+'_dockq_fnat_scores.csv')\n",
    "    for model_id, dockq, fnat, irms, lrms, f1 in results:\n",
    "            # print(f\"Writing row: {model_id}, {dockq}, {dockq_f1}, {fnat}\")\n",
    "            # writer.writerow([model_id, dockq, dockq_f1, fnat])\n",
    "        print(f\"Writing row: {model_id}, {dockq}, {fnat}, {irms},{lrms},{f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d73c04-f0d8-465d-a721-ba3f2bbb3c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### dockq and fnat calculation running\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "for i in complex_list:\n",
    "    fpath = f\"{folder_path}/{i}\"\n",
    "    directory = os.path.expanduser(fpath)\n",
    "    original_directory = os.path.expanduser(original_directory)\n",
    "    main(directory, original_directory)\n",
    "\n",
    "def combine_csv_files(result_path, output_file=None):\n",
    "    # Find all CSV files in the folder\n",
    "    csv_files = glob.glob(os.path.join(result_path, \"*dockq_fnat_scores.csv\"))\n",
    "    \n",
    "    # Read and combine all CSVs\n",
    "    df_list = [pd.read_csv(file) for file in csv_files]\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Save to CSV if an output file is provided\n",
    "    if output_file:\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined CSV saved to {output_file}\")\n",
    "    return combined_df\n",
    "combined_df = combine_csv_files(result_path, dockq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c0476-b73c-47fc-9d56-49230ec2b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdockq2 funcs\n",
    "from Bio.PDB import PDBIO\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio.PDB.Selection import unfold_entities\n",
    "\n",
    "import numpy as np\n",
    "import sys,os\n",
    "import argparse\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def retrieve_IFplddt(structure, chain1, chain2_lst, max_dist):\n",
    "    ## generate a dict to save IF_res_id\n",
    "    chain_lst = list(chain1) + chain2_lst\n",
    "\n",
    "    ifplddt = []\n",
    "    contact_chain_lst = []\n",
    "    for res1 in structure[0][chain1]:\n",
    "        for chain2 in chain2_lst:\n",
    "            count = 0\n",
    "            for res2 in structure[0][chain2]:\n",
    "\n",
    "                if res1.has_id('CA') and res2.has_id('CA'):\n",
    "                   dis = abs(res1['CA']-res2['CA'])\n",
    "                   ## add criteria to filter out disorder res\n",
    "                   if dis <= max_dist:\n",
    "                      ifplddt.append(res1['CA'].get_bfactor())\n",
    "                      count += 1\n",
    "\n",
    "                elif res1.has_id('CB') and res2.has_id('CB'):\n",
    "                   dis = abs(res1['CB']-res2['CB'])\n",
    "                   if dis <= max_dist:\n",
    "                      ifplddt.append(res1['CB'].get_bfactor())\n",
    "                      count += 1\n",
    "            if count > 0:\n",
    "              contact_chain_lst.append(chain2)\n",
    "    contact_chain_lst = sorted(list(set(contact_chain_lst)))   \n",
    "\n",
    "    if len(ifplddt)>0:\n",
    "       IF_plddt_avg = np.mean(ifplddt)\n",
    "    else:\n",
    "       IF_plddt_avg = 0\n",
    "    # print(IF_plddt_avg)\n",
    "\n",
    "    return IF_plddt_avg, contact_chain_lst\n",
    "\n",
    "\n",
    "def retrieve_IFPAEinter(structure, paeMat, contact_lst, max_dist):\n",
    "    ## contact_lst:the chain list that have an interface with each chain. For eg, a tetramer with A,B,C,D chains and A/B A/C B/D C/D interfaces,\n",
    "    ##             contact_lst would be [['B','C'],['A','D'],['A','D'],['B','C']]\n",
    "\n",
    "    chain_lst = [x.id for x in structure[0]]\n",
    "    seqlen = [len(x) for x in structure[0]]\n",
    "    ifch1_col=[]\n",
    "    ifch2_col=[]\n",
    "    ch1_lst=[]\n",
    "    ch2_lst=[]\n",
    "    ifpae_avg = []\n",
    "    d=10\n",
    "    for ch1_idx in range(len(chain_lst)):\n",
    "      ## extract x axis range from the PAE matrix\n",
    "      idx = chain_lst.index(chain_lst[ch1_idx])\n",
    "      ch1_sta=sum(seqlen[:idx])\n",
    "      ch1_end=ch1_sta+seqlen[idx]\n",
    "      ifpae_col = []   \n",
    "      ## for each chain that shares an interface with chain1, retrieve the PAE matrix for the specific part.\n",
    "      for contact_ch in contact_lst[ch1_idx]:\n",
    "        index = chain_lst.index(contact_ch)\n",
    "        ch_sta = sum(seqlen[:index])\n",
    "        ch_end = ch_sta+seqlen[index]\n",
    "        paeMat = np.array(paeMat)\n",
    "        remain_paeMatrix = paeMat[ch1_sta:ch1_end,ch_sta:ch_end]   \n",
    "\n",
    "        ## get avg PAE values for the interfaces for chain 1\n",
    "        mat_x = -1\n",
    "        for res1 in structure[0][chain_lst[ch1_idx]]:\n",
    "          mat_x += 1\n",
    "          mat_y = -1\n",
    "          for res2 in structure[0][contact_ch]:\n",
    "              mat_y+=1\n",
    "              if res1['CA'] - res2['CA'] <=max_dist:\n",
    "                 ifpae_col.append(remain_paeMatrix[mat_x,mat_y])\n",
    "      ## normalize by d(10A) first and then get the average\n",
    "      if not ifpae_col:\n",
    "        ifpae_avg.append(0)\n",
    "      else:\n",
    "        norm_if_interpae=np.mean(1/(1+(np.array(ifpae_col)/d)**2))\n",
    "        ifpae_avg.append(norm_if_interpae)\n",
    "        # print(ifpae_avg)\n",
    "    return ifpae_avg\n",
    "\n",
    "def calc_pmidockq(ifpae_norm, ifplddt):\n",
    "    df = pd.DataFrame()\n",
    "    df['ifpae_norm'] = ifpae_norm\n",
    "    df['ifplddt'] = ifplddt\n",
    "\n",
    "    df['prot'] = df.ifpae_norm*df.ifplddt\n",
    "    fitpopt = [1.31034849e+00, 8.47326239e+01, 7.47157696e-02, 5.01886443e-03] ## from orignal pdcokq2 fit  \n",
    "    df['pmidockq'] = sigmoid(df.prot.values, *fitpopt)\n",
    "    return df\n",
    "\n",
    "def sigmoid(x, L ,x0, k, b):\n",
    "    y = L / (1 + np.exp(-k*(x-x0)))+b\n",
    "    return (y)\n",
    "\n",
    "def process_pdb_file(pdb_file, json_file, distance, file_id, chains_part=\"\", pae=None):\n",
    "    \n",
    "    # Parse the PDB file\n",
    "    pdbp = PDBParser(QUIET=True)\n",
    "    structure = pdbp.get_structure('', pdb_file)\n",
    "    chains = [chain.id for chain in structure[0]]\n",
    "\n",
    "    remain_contact_lst = []\n",
    "    plddt_lst = []\n",
    "\n",
    "    # Assuming these functions are defined in the same file or imported\n",
    "    for idx in range(len(chains)):\n",
    "        chain2_lst = list(set(chains)-set(chains[idx]))\n",
    "        IF_plddt, contact_lst = retrieve_IFplddt(structure, chains[idx], chain2_lst, distance)\n",
    "        plddt_lst.append(IF_plddt)\n",
    "        remain_contact_lst.append(contact_lst)\n",
    "    \n",
    "    # Use the passed 'pae' directly, no need to read the JSON file again\n",
    "    avgif_pae = retrieve_IFPAEinter(structure, pae, remain_contact_lst, distance)\n",
    "\n",
    "    # Calculate the results using the processed data\n",
    "    res = calc_pmidockq(avgif_pae, plddt_lst)\n",
    "\n",
    "    # Extract the PDB ID from the filename\n",
    "    pdb_id = os.path.basename(pdb_file).split('_')[0]\n",
    "    \n",
    "    # Prepare the result dictionary\n",
    "    result = {\n",
    "        \"model_id\" : pdb_file,\n",
    "        \"pdb_id\": file_id,\n",
    "        \"pdb_id_with_chains\": '{0}_{1}'.format(pdb_id, chains_part),\n",
    "        \"ipae_norm_ag\": res['ifpae_norm'].tolist()[-1],\n",
    "        \"ipae_norm_avg\": np.mean(res['ifpae_norm']), \n",
    "        \"iplddt_ag\": res['ifplddt'].tolist()[-1],\n",
    "        \"iplddt_avg\": np.mean(res['ifplddt']), \n",
    "        \"pDockQ2_ag\": res['pmidockq'].tolist()[-1],\n",
    "        \"pDockQ2_avg\": np.mean(res['pmidockq'])}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80709580-0af2-4602-8006-a4ab47f03981",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def find_matching_json(pdb_file, directory):\n",
    "    pdb_file_basename = os.path.basename(pdb_file)\n",
    "    \n",
    "    # Use a regular expression to extract the model index from the filename\n",
    "    import re\n",
    "    match = re.search(r'model_idx_(\\d+)', pdb_file_basename)\n",
    "    \n",
    "    if match:\n",
    "        model_idx = match.group(1)  # Extracted model index\n",
    "        print(f\"Extracted model index: {model_idx}\")\n",
    "    else:\n",
    "        print(f\"Could not extract model index from: {pdb_file_basename}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract pdb_id and chains_part from the PDB file name\n",
    "    parts = pdb_file_basename.split('_')\n",
    "    pdb_id = parts[0]\n",
    "    chains_part = \"_\".join(parts[1:parts.index(\"chai\")])  # Use \"chai\" as identifier\n",
    "    \n",
    "    json_pattern = f\"scores.model_idx_{model_idx}.json\"  \n",
    "    print(f\"Looking for JSON file with pattern: {json_pattern}\")\n",
    "\n",
    "    json_files = glob.glob(os.path.join(directory, json_pattern))\n",
    "    \n",
    "    print(f\"Found JSON files: {json_files}\")\n",
    "    \n",
    "    return json_files[0] if json_files else None \n",
    "    \n",
    "def run_processing(directory):\n",
    "    pdb_files = glob.glob(os.path.join(directory, \"*_unrelaxed*.pdb\"))\n",
    "    print(f\"Found PDB files: {pdb_files}\")  \n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_file_basename = os.path.basename(pdb_file)\n",
    "        parts = pdb_file_basename.split('_')\n",
    "        pdb_id = parts[0]\n",
    "        chains_part = \"_\".join(parts[1:parts.index(\"chai\")])  \n",
    "\n",
    "        # Find the matching JSON file\n",
    "        json_file_name = find_matching_json(pdb_file, directory)\n",
    "        \n",
    "        if json_file_name and os.path.exists(json_file_name):\n",
    "  \n",
    "            with open(json_file_name, 'r') as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "                pae = json_data.get('pae', None)  \n",
    "            \n",
    "            if pae is not None:\n",
    "                file_id = f\"{pdb_id}_{chains_part}\"\n",
    "                result = process_pdb_file(pdb_file, json_file_name, 8, pdb_id, chains_part, pae)  \n",
    "                print(f\"Result for {pdb_file}: {result}\")\n",
    "                results.append(result)\n",
    "            else:\n",
    "                print(f\"PAE not found in JSON file for {pdb_file}.\")\n",
    "        else:\n",
    "            print(f\"JSON file for {pdb_file} not found.\")\n",
    "\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        csv_file_path = f\"{file_id}_pdockq2_fit.csv\"  \n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "        print(f\"Data saved to {csv_file_path}\")\n",
    "    else:\n",
    "        print(\"No results to save.\")\n",
    "\n",
    "### run for all predictions\n",
    "for i in complex_list:\n",
    "    directory = f\"{folder_path}/{i}\"\n",
    "    run_processing(directory)   \n",
    "\n",
    "def combine_csv_files(result_path, output_file=None):\n",
    "    csv_files = glob.glob(os.path.join(result_path, \"*_pdockq2_fit.csv\"))\n",
    "    df_list = [pd.read_csv(file) for file in csv_files]\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    if output_file:\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined CSV saved to {output_file}\")\n",
    "    return combined_df\n",
    "    \n",
    "combined_df = combine_csv_files(result_path, pdockq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a1a08-e636-4f1c-a542-67608ecb0d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "x = []\n",
    "\n",
    "for i in complex_list:\n",
    "    os.chdir(f\"{folder_path}/{i}\")    \n",
    "    df = pd.read_csv(f\"{folder_path}/{i}/metrics.csv\")    \n",
    "    df['model_id'] = df['filename'].str.replace(\"scores.\",f\"{f}_\").str.replace(\"_idx\",\"\")\n",
    "    df['model_id'] = df['model_id'].str.replace(\".json\",\"\")\n",
    "    df = df[[\"model_id\",'aggregate_score', 'ptm', 'iptm']]    \n",
    "    x.append(df)\n",
    "  \n",
    "dfy = pd.concat(x, axis=0)\n",
    "# print(dfy.head())\n",
    "\n",
    "df = pd.read_csv(f\"{result_path}/{pdockq_output}\")\n",
    "df['model_id']=df['model_id'].str.replace(\".pdb\",\"\").str.replace(\"unrelaxedpred.\",\"\").str.replace(\"_idx\",\"\")\n",
    "df['model_id']=df['model_id'].str.split(\"/\").str[-1]\n",
    "# print(df.head())\n",
    "\n",
    "df2 = pd.read_csv(f\"{result_path}/{dockq_output}\")\n",
    "df2['model_id']=df2['model_id'].str.replace(\".pdb\",\"\").str.replace(\"unrelaxedpred.\",\"\").str.replace(\"_idx\",\"\")\n",
    "print(df2.head())\n",
    "dfx = pd.merge(df,df2,on=\"model_id\")\n",
    "\n",
    "df_fin = pd.merge(dfx,dfy,on=\"model_id\")\n",
    "df_fin=df_fin.drop_duplicates()\n",
    "\n",
    "# AntiConf = 0.3pDockQ2_ag + 0.7pTM\n",
    "df_fin[\"AntiConf\"] =(0.3 * df_fin[\"pDockQ2_ag\"]) + (0.7 * df_fin[\"ptm\"])\n",
    "df_fin.to_csv(f\"{result_path}/{combined}\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
