{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2947597c-84ae-45fe-9fa5-4ff97088c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pred_method = \"af2\" ## method\n",
    "folder_path = f\"{pred_method}\"\n",
    "## file hierarchy should be like this\n",
    "# af2\n",
    "# ├── 7ar0_B_A_af2\n",
    "# │   ├── 7ar0_B_A_af2.a3m\n",
    "# │   ├── 7ar0_B_A_af2.png\n",
    "# │   ├── 7ar0_B_A_af2.png\n",
    "# │   ├── 7ar0_B_A_af2.png\n",
    "# │   ├── ...\n",
    "# ├── 7bnv_H_L_A_af2\n",
    "# ├── ...\n",
    "\n",
    "complex_list = [\n",
    "    f for f in os.listdir(folder_path)\n",
    "    if os.path.isdir(os.path.join(folder_path, f)) and not f.startswith(\".\")\n",
    "]\n",
    "print(complex_list)  # should print prediction folders ['7ar0_B_A_af2', '...', '...']\n",
    "\n",
    "original_directory = \"path/to/native_PDBs\" ## Native PDB directory\n",
    "\n",
    "## create results folder\n",
    "result_path = \"path/to/results_folder\"\n",
    "dockq_output = f\"{pred_method}_dockq_fnat_scores.csv\"  ## dockq output\n",
    "pdockq_output = f\"{pred_method}_pdockq2_fit.csv\" ## pdockq2 output\n",
    "combined = f\"{pred_method}_combined.csv\" ### combined results with dockq_pdock2 and model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c36e8-b846-4a8e-93e1-bb76a6e37c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run dockq function\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import glob\n",
    "from DockQ.DockQ import load_PDB, run_on_all_native_interfaces\n",
    "from statistics import mean\n",
    "\n",
    "def merge_chains(model, chains_to_merge):\n",
    "    \"\"\"\n",
    "    Merges specified chains in the given model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Bio.PDB.Structure\n",
    "        The model in which the chains are to be merged.\n",
    "    chains_to_merge : list of str\n",
    "        The list of chain IDs to be merged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : Bio.PDB.Structure\n",
    "        The model with the specified chains merged.\n",
    "    \"\"\"\n",
    "    print(f\"Merging chains {chains_to_merge} in model\")\n",
    "    for chain in chains_to_merge[1:]:\n",
    "        for res in list(model[chain]):\n",
    "            res.id = (chains_to_merge[0], res.id[1], res.id[2])\n",
    "            model[chains_to_merge[0]].add(res)\n",
    "        model.detach_child(chain)\n",
    "    model[chains_to_merge[0]].id = \"\".join(chains_to_merge)\n",
    "    return model\n",
    "\n",
    "def calculate_dockq(model, native, chain_map):\n",
    "    \"\"\"\n",
    "    Calculates DockQ scores for the given model and native structures based on the chain map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Bio.PDB.Structure\n",
    "        The model structure.\n",
    "    native : Bio.PDB.Structure\n",
    "        The native structure.\n",
    "    chain_map : dict\n",
    "        The mapping of chains between model and native structures.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        The results containing various DockQ metrics.\n",
    "    dockq_score : float\n",
    "        The DockQ score.\n",
    "    \"\"\"\n",
    "    print(f\"Calculating DockQ score with chain_map: {chain_map}\")\n",
    "    results, dockq_score = run_on_all_native_interfaces(model, native, chain_map=chain_map)\n",
    "    return results, dockq_score\n",
    "\n",
    "def process_models(models):\n",
    "    \"\"\"\n",
    "    Processes the provided models and calculates DockQ scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list of tuple\n",
    "        List of tuples where each tuple contains the model file path and native file path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results_list : list of tuple\n",
    "        List of tuples containing model_id, DockQ, DockQ_F1, and fnat scores.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    for model_file, native_file in models:\n",
    "        print(f\"Processing model: {model_file}, native: {native_file}\")\n",
    "        model_id = os.path.basename(model_file).split(\".\")[0]\n",
    "        model = load_PDB(model_file)\n",
    "        native = load_PDB(native_file)\n",
    "\n",
    "        chain_ids = list(model.child_dict.keys())\n",
    "        print(chain_ids)\n",
    "        native_chain_ids = list(native.child_dict.keys())\n",
    "\n",
    "        if len(chain_ids) == 3:\n",
    "            print(f\"Model {model_id} has 3 chains: {chain_ids}\")\n",
    "            \n",
    "            model_merged = merge_chains(model, chain_ids[:2])\n",
    "            print(model_merged)\n",
    "            native_merged = merge_chains(native, native_chain_ids[:2])\n",
    "            chain_map_merged = {native_chain_ids[2]: chain_ids[2], \"\".join(native_chain_ids[:2]): \"\".join(chain_ids[:2])}\n",
    "            results_merged, dockq_score_merged = calculate_dockq(model_merged, native_merged, chain_map_merged)\n",
    "            merged_result = results_merged[list(results_merged.keys())[0]]\n",
    "            results_list.append((model_id, merged_result['DockQ'], merged_result['fnat'],\n",
    "                                merged_result['iRMSD'], merged_result['LRMSD'],merged_result['F1']))\n",
    "\n",
    "        elif len(chain_ids) == 2:\n",
    "            print(f\"Model {model_id} has 2 chains: {chain_ids}\")\n",
    "            # Assume there are only 2 chains\n",
    "            chain_map = {native_chain_ids[0]: chain_ids[0], native_chain_ids[1]: chain_ids[1]}\n",
    "            results, dockq_score = calculate_dockq(model, native, chain_map)\n",
    "            results_list.append((model_id, results[list(results.keys())[0]]['DockQ'], results[list(results.keys())[0]]['fnat'],\n",
    "                                results[list(results.keys())[0]]['iRMSD'], results[list(results.keys())[0]]['LRMSD'],\n",
    "                                results[list(results.keys())[0]]['F1']))\n",
    "\n",
    "        else:\n",
    "            print(f\"Model {model_id} does not have 2 or 3 chains: {chain_ids}, skipping.\")\n",
    "            continue\n",
    "    return results_list\n",
    "\n",
    "def save_results_to_csv(results, filename):\n",
    "    print(f\"Saving results to CSV file: {filename}\")\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['model_id', 'DockQ', 'fnat',\"iRMSD\",\"LRMSD\",\"F1\"])\n",
    "\n",
    "        for model_id, dockq, fnat, irms, lrms, f1 in results:\n",
    "            print(f\"Writing row: {model_id}, {dockq}, {fnat}, {irms},{lrms},{f1}\")\n",
    "            writer.writerow([model_id, dockq, fnat, irms, lrms,f1])   \n",
    "\n",
    "def main(directory, original_directory):\n",
    "    \"\"\"\n",
    "    Main function to find PDB files, process models, and save results to a CSV file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        Path to the directory containing PDB files to be analyzed.\n",
    "    original_directory : str\n",
    "        Path to the directory containing original PDB files for comparison.\n",
    "\n",
    "    Returns\n",
    "    -------                                                                                                                                                                                                                                                                                                                                    \n",
    "    None\n",
    "    \"\"\"\n",
    "    pdb_files = glob.glob(f\"{directory}/*_unrelaxed_rank_00*.pdb\")\n",
    "    if not pdb_files:\n",
    "        print(f\"No PDB files found in directory: {directory}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found PDB files: {pdb_files}\")\n",
    "    models = []\n",
    "\n",
    "    for pdb_file in pdb_files:\n",
    "        pdb_id_chains = os.path.basename(pdb_file).split('_unrelaxed')[0] # for af2.3\n",
    "        pdb_id_chains = \"_\".join(pdb_id_chains.split(\"_\")[:-1])\n",
    "        print(f\"Searching for original files for: {pdb_id_chains}\")\n",
    "        original_pdb_files = glob.glob(f\"{original_directory}/{pdb_id_chains}.pdb\")  # Find matching original PDB files\n",
    "        print(f\"Found original PDB files: {original_pdb_files} for {pdb_file}\")\n",
    "        if not original_pdb_files:\n",
    "            print(f\"No matching original PDB file found for {pdb_file}, skipping.\")\n",
    "            continue\n",
    "        for original_pdb_file in original_pdb_files:\n",
    "            models.append((pdb_file, original_pdb_file))\n",
    "            print(f\"Adding model-native pair: {pdb_file}, {original_pdb_file}\")\n",
    "\n",
    "    if not models:\n",
    "        print(\"No valid model-native pairs found. Exiting.\")\n",
    "        return\n",
    "    os.chdir(result_path)\n",
    "    results = process_models(models)\n",
    "    save_results_to_csv(results, str(pdb_id_chains)+'_dockq_fnat_scores.csv')\n",
    "    for model_id, dockq, fnat, irms, lrms, f1 in results:\n",
    "        print(f\"Writing row: {model_id}, {dockq}, {fnat}, {irms},{lrms},{f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacb432-be8c-4a49-828f-e29d4027b841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "for i in complex_list:\n",
    "    fpath = f\"{folder_path}/{i}\"\n",
    "    directory = os.path.expanduser(fpath)\n",
    "    original_directory = os.path.expanduser(original_directory)\n",
    "    main(directory, original_directory)\n",
    "\n",
    "def combine_csv_files(result_path, output_file=None):\n",
    "    # Find all CSV files in the folder\n",
    "    csv_files = glob.glob(os.path.join(result_path, \"*dockq_fnat_scores.csv\"))\n",
    "    \n",
    "    # Read and combine all CSVs\n",
    "    df_list = [pd.read_csv(file) for file in csv_files]\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Save to CSV if an output file is provided\n",
    "    if output_file:\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined CSV saved to {output_file}\")\n",
    "    return combined_df\n",
    "combined_df = combine_csv_files(result_path, dockq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da717b-f8f3-4132-b100-7d0df589d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pdockq2 functions\n",
    "\n",
    "from Bio.PDB import PDBIO\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio.PDB.Selection import unfold_entities\n",
    "import numpy as np\n",
    "import sys,os\n",
    "import argparse\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def retrieve_IFplddt(structure, chain1, chain2_lst, max_dist):\n",
    "    chain_lst = list(chain1) + chain2_lst\n",
    "    ifplddt = []\n",
    "    contact_chain_lst = []\n",
    "    for res1 in structure[0][chain1]:\n",
    "        for chain2 in chain2_lst:\n",
    "            count = 0\n",
    "            for res2 in structure[0][chain2]:\n",
    "                if res1.has_id('CA') and res2.has_id('CA'):\n",
    "                   dis = abs(res1['CA']-res2['CA'])\n",
    "                   ## add criteria to filter out disorder res\n",
    "                   if dis <= max_dist:\n",
    "                      ifplddt.append(res1['CA'].get_bfactor())\n",
    "                      count += 1\n",
    "                elif res1.has_id('CB') and res2.has_id('CB'):\n",
    "                   dis = abs(res1['CB']-res2['CB'])\n",
    "                   if dis <= max_dist:\n",
    "                      ifplddt.append(res1['CB'].get_bfactor())\n",
    "                      count += 1\n",
    "            if count > 0:\n",
    "              contact_chain_lst.append(chain2)\n",
    "    contact_chain_lst = sorted(list(set(contact_chain_lst)))   \n",
    "    if len(ifplddt)>0:\n",
    "       IF_plddt_avg = np.mean(ifplddt)\n",
    "    else:\n",
    "       IF_plddt_avg = 0\n",
    "    return IF_plddt_avg, contact_chain_lst\n",
    "\n",
    "\n",
    "def retrieve_IFPAEinter(structure, paeMat, contact_lst, max_dist):\n",
    "    chain_lst = [x.id for x in structure[0]]\n",
    "    seqlen = [len(x) for x in structure[0]]\n",
    "    ifch1_col=[]\n",
    "    ifch2_col=[]\n",
    "    ch1_lst=[]\n",
    "    ch2_lst=[]\n",
    "    ifpae_avg = []\n",
    "    d=10\n",
    "    for ch1_idx in range(len(chain_lst)):\n",
    "      idx = chain_lst.index(chain_lst[ch1_idx])\n",
    "      ch1_sta=sum(seqlen[:idx])\n",
    "      ch1_end=ch1_sta+seqlen[idx]\n",
    "      ifpae_col = []   \n",
    "      for contact_ch in contact_lst[ch1_idx]:\n",
    "        index = chain_lst.index(contact_ch)\n",
    "        ch_sta = sum(seqlen[:index])\n",
    "        ch_end = ch_sta+seqlen[index]\n",
    "        paeMat = np.array(paeMat)\n",
    "        remain_paeMatrix = paeMat[ch1_sta:ch1_end,ch_sta:ch_end]\n",
    "        mat_x = -1\n",
    "        for res1 in structure[0][chain_lst[ch1_idx]]:\n",
    "          mat_x += 1\n",
    "          mat_y = -1\n",
    "          for res2 in structure[0][contact_ch]:\n",
    "              mat_y+=1\n",
    "              if res1['CA'] - res2['CA'] <=max_dist:\n",
    "                 ifpae_col.append(remain_paeMatrix[mat_x,mat_y])\n",
    "      if not ifpae_col:\n",
    "        ifpae_avg.append(0)\n",
    "      else:\n",
    "        norm_if_interpae=np.mean(1/(1+(np.array(ifpae_col)/d)**2))\n",
    "        ifpae_avg.append(norm_if_interpae)\n",
    "    return ifpae_avg\n",
    "\n",
    "def calc_pmidockq(ifpae_norm, ifplddt):\n",
    "    df = pd.DataFrame()\n",
    "    df['ifpae_norm'] = ifpae_norm\n",
    "    df['ifplddt'] = ifplddt\n",
    "    df['prot'] = df.ifpae_norm*df.ifplddt\n",
    "    fitpopt = [1.31034849e+00, 8.47326239e+01, 7.47157696e-02, 5.01886443e-03] ## from orignal fit function  \n",
    "    df['pmidockq'] = sigmoid(df.prot.values, *fitpopt)\n",
    "    return df\n",
    "\n",
    "def sigmoid(x, L ,x0, k, b):\n",
    "    y = L / (1 + np.exp(-k*(x-x0)))+b\n",
    "    return (y)\n",
    "\n",
    "def process_pdb_file(pdb_file, json_file, distance, file_id, chains_part=\"\"): \n",
    "    pdbp = PDBParser(QUIET=True)\n",
    "    structure = pdbp.get_structure('', pdb_file)\n",
    "    chains = [chain.id for chain in structure[0]]\n",
    "    remain_contact_lst = []\n",
    "    plddt_lst = []\n",
    "    for idx in range(len(chains)):\n",
    "        chain2_lst = list(set(chains)-set(chains[idx]))\n",
    "        IF_plddt, contact_lst = retrieve_IFplddt(structure, chains[idx], chain2_lst, distance)\n",
    "        plddt_lst.append(IF_plddt)\n",
    "        remain_contact_lst.append(contact_lst)\n",
    "    pae_data = pd.read_json(json_file, lines=True)\n",
    "    avgif_pae = retrieve_IFPAEinter(structure, pae_data[\"pae\"][0], remain_contact_lst, distance)\n",
    "    res = calc_pmidockq(avgif_pae, plddt_lst)\n",
    "    pdb_id = os.path.basename(pdb_file).split('_')[0]\n",
    "    result = {\n",
    "        \"model_id\" : pdb_file,\n",
    "        \"pdb_id\": file_id,\n",
    "        \"pdb_id_with_chains\": '{0}_{1}'.format(pdb_id, chains_part),\n",
    "        \"ipae_norm_ag\": res['ifpae_norm'].tolist()[-1],\n",
    "        \"ipae_norm_avg\": np.mean(res['ifpae_norm']), \n",
    "        \"iplddt_ag\": res['ifplddt'].tolist()[-1],\n",
    "        \"iplddt_avg\": np.mean(res['ifplddt']), \n",
    "        \"pDockQ2_ag\": res['pmidockq'].tolist()[-1],\n",
    "        \"pDockQ2_avg\": np.mean(res['pmidockq'])}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c17447-6b5f-4150-9f81-1911ab50a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def find_matching_json(pdb_file, directory):\n",
    "    pdb_file_basename = os.path.basename(pdb_file)\n",
    "    parts = pdb_file_basename.split('_')\n",
    "    pdb_id = parts[0]\n",
    "    chains_part = \"_\".join(parts[1:parts.index([p for p in parts if p.startswith('unrelaxed')][0])])\n",
    "    pattern = f\"{pdb_id}_{chains_part}_scores_rank_00*.json\"\n",
    "    json_files = glob.glob(os.path.join(directory, pattern))\n",
    "    return json_files[0] if json_files else print(pattern + \" not found\")\n",
    "\n",
    "def run_processing(directory):\n",
    "    pdb_files = glob.glob(os.path.join(directory, \"*_unrelaxed_rank_00*.pdb\"))\n",
    "    results = []   \n",
    "    for pdb_file in pdb_files:\n",
    "        base_name = pdb_file.rsplit('_', 4)[0]\n",
    "        pdb_file_basename = os.path.basename(pdb_file)\n",
    "        parts = pdb_file_basename.split('_')\n",
    "        pdb_id = parts[0]\n",
    "        chains_part = \"_\".join(parts[1:parts.index([p for p in parts if p.startswith('unrelaxed')][0])])\n",
    "        json_file_name = find_matching_json(pdb_file, directory)\n",
    "        if os.path.exists(json_file_name):\n",
    "            print(f\"{pdb_id}_{chains_part}\")\n",
    "            file_id = f\"{pdb_id}_{chains_part}\"\n",
    "            result = process_pdb_file(pdb_file, json_file_name, 8, pdb_id, chains_part)  # Adjust Ca-ca distance\n",
    "            print(f\"Result for {pdb_file}: {result}\")  \n",
    "            results.append(result)\n",
    "        else:\n",
    "            print(f\"JSON file for {pdb_file} not found.\")\n",
    "            return\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        os.chdir(result_path)\n",
    "        csv_file_path = str(file_id)+\"_pdockq2_fit.csv\"\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "        print(f\"Data saved to {csv_file_path}\")\n",
    "    else:\n",
    "        print(\"No results to save.\")\n",
    "\n",
    "### run for all predictions\n",
    "for i in complex_list:\n",
    "    fpath = f\"{folder_path}/{i}\"\n",
    "    directory = fpath\n",
    "    run_processing(directory)   \n",
    "\n",
    "def combine_csv_files(result_path, output_file=None):\n",
    "    csv_files = glob.glob(os.path.join(result_path, \"*_pdockq2_fit.csv\"))\n",
    "    df_list = [pd.read_csv(file) for file in csv_files]\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    if output_file:\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined CSV saved to {output_file}\")\n",
    "    return combined_df\n",
    "    \n",
    "combined_df = combine_csv_files(result_path, pdockq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb205a03-e7f9-4219-842a-19a3a2362cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract log metrics\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_log_file(log_file):\n",
    "    data = []\n",
    "    query_pdb = None\n",
    "\n",
    "    with open(log_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            match_query = re.search(r'Query \\d+/\\d+: (\\S+) ', line)\n",
    "            if match_query:\n",
    "                query_pdb = match_query.group(1)\n",
    "            match_model = re.search(r'(rank_\\d+_alphafold2_multimer_v3_model_\\d+_seed_\\d+) '\n",
    "                                    r'pLDDT=([\\d.]+) pTM=([\\d.]+) ipTM=([\\d.]+) actifpTM=([\\d.]+)', line)\n",
    "            if match_model and query_pdb:\n",
    "                model_name = match_model.group(1)\n",
    "                scores = list(map(float, match_model.groups()[1:]))  \n",
    "                data.append([query_pdb, model_name] + scores)\n",
    "    df = pd.DataFrame(data, columns=[\"Query_PDB\", \"Model_Name\", \"pLDDT\", \"pTM\", \"ipTM\",\"actifpTM\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bd196-c05c-4ca2-aa37-749dd2c45b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### combine all files\n",
    "\n",
    "x=[]\n",
    "for i in complex_list:\n",
    "    df = parse_log_file(f\"{folder_path}/{i}/log.txt\")\n",
    "    x.append(df)\n",
    "dfy = pd.concat(x)\n",
    "\n",
    "dfy['Model_Name']=dfy['Model_Name'].str.replace(\"rank\",\"unrelaxed_rank\")\n",
    "dfy['model_id'] = dfy['Query_PDB'] + \"_\"+dfy['Model_Name']\n",
    "dfy['model_id']\n",
    "dfy=dfy[[\"model_id\",\"pLDDT\",\"pTM\",\"ipTM\",\"actifpTM\"]]\n",
    "print(list(dfy))\n",
    "\n",
    "df = pd.read_csv(f\"{result_path}/{pdockq_output}\")\n",
    "df['model_id']=df['model_id'].str.split(\"/\").str[-1]\n",
    "df['model_id']=df['model_id'].str.replace(\".pdb\",\"\")\n",
    "\n",
    "df2 = pd.read_csv(f\"{result_path}/{dockq_output}\")\n",
    "\n",
    "dfx = pd.merge(df,df2,on=\"model_id\")\n",
    "\n",
    "df_fin = pd.merge(dfx,dfy,on=\"model_id\")\n",
    "df_fin=df_fin.drop_duplicates()\n",
    "print(list(df))\n",
    "df_fin[\"pdb_id_wchains\"]=df_fin[\"pdb_id_with_chains\"].str.replace(\"_af2\",\"\")\n",
    "df_fin=df_fin[['model_id', 'pdb_id', 'pdb_id_wchains', 'ipae_norm_ag', 'ipae_norm_avg', 'iplddt_ag', 'iplddt_avg', \n",
    "               'pDockQ2_ag', 'pDockQ2_avg','pLDDT', 'pTM', 'ipTM', 'actifpTM']]\n",
    "\n",
    "# AntiConf = 0.3pDockQ2_ag + 0.7pTM\n",
    "df_fin[\"AntiConf\"] = 0.3* df_fin[\"pDockQ2_ag\"] + 0.7 * df_fin[\"pTM\"]\n",
    "df_fin.to_csv(f\"{result_path}/{combined}\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
